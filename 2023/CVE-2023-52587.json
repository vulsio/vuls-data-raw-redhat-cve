{
	"name": "CVE-2023-52587",
	"threat_severity": "Moderate",
	"public_date": "2024-03-06T00:00:00Z",
	"bugzilla": {
		"description": "kernel: IB/ipoib: Fix mcast list locking",
		"id": "2268331",
		"url": "https://bugzilla.redhat.com/show_bug.cgi?id=2268331"
	},
	"cvss3": {
		"cvss3_base_score": "5.5",
		"cvss3_scoring_vector": "CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H",
		"status": "draft"
	},
	"cwe": "CWE-413",
	"details": [
		"In the Linux kernel, the following vulnerability has been resolved:\nIB/ipoib: Fix mcast list locking\nReleasing the `priv->lock` while iterating the `priv->multicast_list` in\n`ipoib_mcast_join_task()` opens a window for `ipoib_mcast_dev_flush()` to\nremove the items while in the middle of iteration. If the mcast is removed\nwhile the lock was dropped, the for loop spins forever resulting in a hard\nlockup (as was reported on RHEL 4.18.0-372.75.1.el8_6 kernel):\nTask A (kworker/u72:2 below)       | Task B (kworker/u72:0 below)\n-----------------------------------+-----------------------------------\nipoib_mcast_join_task(work)        | ipoib_ib_dev_flush_light(work)\nspin_lock_irq(&priv->lock)       | __ipoib_ib_dev_flush(priv, ...)\nlist_for_each_entry(mcast,       | ipoib_mcast_dev_flush(dev = priv->dev)\n&priv->multicast_list, list) |\nipoib_mcast_join(dev, mcast)   |\nspin_unlock_irq(&priv->lock) |\n|   spin_lock_irqsave(&priv->lock, flags)\n|   list_for_each_entry_safe(mcast, tmcast,\n|                  &priv->multicast_list, list)\n|     list_del(&mcast->list);\n|     list_add_tail(&mcast->list, &remove_list)\n|   spin_unlock_irqrestore(&priv->lock, flags)\nspin_lock_irq(&priv->lock)   |\n|   ipoib_mcast_remove_list(&remove_list)\n(Here, `mcast` is no longer on the  |     list_for_each_entry_safe(mcast, tmcast,\n`priv->multicast_list` and we keep |                            remove_list, list)\nspinning on the `remove_list` of   |  >>>  wait_for_completion(&mcast->done)\nthe other thread which is blocked  |\nand the list is still valid on     |\nit's stack.)\nFix this by keeping the lock held and changing to GFP_ATOMIC to prevent\neventual sleeps.\nUnfortunately we could not reproduce the lockup and confirm this fix but\nbased on the code review I think this fix should address such lockups.\ncrash> bc 31\nPID: 747      TASK: ff1c6a1a007e8000  CPU: 31   COMMAND: \"kworker/u72:2\"\n--\n[exception RIP: ipoib_mcast_join_task+0x1b1]\nRIP: ffffffffc0944ac1  RSP: ff646f199a8c7e00  RFLAGS: 00000002\nRAX: 0000000000000000  RBX: ff1c6a1a04dc82f8  RCX: 0000000000000000\nwork (&priv->mcast_task{,.work})\nRDX: ff1c6a192d60ac68  RSI: 0000000000000286  RDI: ff1c6a1a04dc8000\n&mcast->list\nRBP: ff646f199a8c7e90   R8: ff1c699980019420   R9: ff1c6a1920c9a000\nR10: ff646f199a8c7e00  R11: ff1c6a191a7d9800  R12: ff1c6a192d60ac00\nmcast\nR13: ff1c6a1d82200000  R14: ff1c6a1a04dc8000  R15: ff1c6a1a04dc82d8\ndev                    priv (&priv->lock)     &priv->multicast_list (aka head)\nORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018\n--- <NMI exception stack> ---\n#5 [ff646f199a8c7e00] ipoib_mcast_join_task+0x1b1 at ffffffffc0944ac1 [ib_ipoib]\n#6 [ff646f199a8c7e98] process_one_work+0x1a7 at ffffffff9bf10967\ncrash> rx ff646f199a8c7e68\nff646f199a8c7e68:  ff1c6a1a04dc82f8 <<< work = &priv->mcast_task.work\ncrash> list -hO ipoib_dev_priv.multicast_list ff1c6a1a04dc8000\n(empty)\ncrash> ipoib_dev_priv.mcast_task.work.func,mcast_mutex.owner.counter ff1c6a1a04dc8000\nmcast_task.work.func = 0xffffffffc0944910 <ipoib_mcast_join_task>,\nmcast_mutex.owner.counter = 0xff1c69998efec000\ncrash> b 8\nPID: 8        TASK: ff1c69998efec000  CPU: 33   COMMAND: \"kworker/u72:0\"\n--\n#3 [ff646f1980153d50] wait_for_completion+0x96 at ffffffff9c7d7646\n#4 [ff646f1980153d90] ipoib_mcast_remove_list+0x56 at ffffffffc0944dc6 [ib_ipoib]\n#5 [ff646f1980153de8] ipoib_mcast_dev_flush+0x1a7 at ffffffffc09455a7 [ib_ipoib]\n#6 [ff646f1980153e58] __ipoib_ib_dev_flush+0x1a4 at ffffffffc09431a4 [ib_ipoib]\n#7 [ff\n---truncated---",
		"A hard lockup flaw was found in the Linux kernelâ€™s IPoIB driver in how a user triggers the ipoib_mcast_join_task() function, caused by invalid priv->multicast_list locking. This flaw allows a local user to crash the system."
	],
	"statement": "Red Hat Enterprise Linux 9 is not affected by this vulnerability.",
	"references": [
		"https://www.cve.org/CVERecord?id=CVE-2023-52587\nhttps://nvd.nist.gov/vuln/detail/CVE-2023-52587\nhttps://lore.kernel.org/linux-cve-announce/2024030644-CVE-2023-52587-5479@gregkh/T"
	],
	"mitigation": {
		"lang": "en:us",
		"value": "To mitigate this issue, prevent module ib_core from being loaded. Please see https://access.redhat.com/solutions/41278 for information on blacklisting a kernel module to prevent it from loading automatically."
	},
	"package_state": [
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:6",
			"fix_state": "Out of support scope",
			"package_name": "kernel",
			"product_name": "Red Hat Enterprise Linux 6"
		},
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:7",
			"fix_state": "Out of support scope",
			"package_name": "kernel",
			"product_name": "Red Hat Enterprise Linux 7"
		},
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:7",
			"fix_state": "Out of support scope",
			"package_name": "kernel-rt",
			"product_name": "Red Hat Enterprise Linux 7"
		},
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:8",
			"fix_state": "Affected",
			"package_name": "kernel",
			"product_name": "Red Hat Enterprise Linux 8"
		},
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:8",
			"fix_state": "Will not fix",
			"package_name": "kernel-rt",
			"product_name": "Red Hat Enterprise Linux 8"
		},
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:9",
			"fix_state": "Not affected",
			"package_name": "kernel",
			"product_name": "Red Hat Enterprise Linux 9"
		},
		{
			"cpe": "cpe:/o:redhat:enterprise_linux:9",
			"fix_state": "Not affected",
			"package_name": "kernel-rt",
			"product_name": "Red Hat Enterprise Linux 9"
		}
	],
	"upstream_fix": "kernel 4.19.307, kernel 5.4.269, kernel 5.10.210, kernel 5.15.149, kernel 6.1.77, kernel 6.6.16, kernel 6.7.4, kernel 6.8-rc1",
	"csaw": false
}
